{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "from models.data_loader import DataLoader\n",
    "\n",
    "from models.retain_copy import RETAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 30\n",
    "batch_size = 50\n",
    "max_seq_length = 300\n",
    "min_seq_length = 5\n",
    "num_classes = 268\n",
    "emb_size = 128\n",
    "hid_size = 128\n",
    "lr = 0.001\n",
    "cuda_flag = True\n",
    "\n",
    "# data loader\n",
    "D = DataLoader(batch_size=batch_size,\n",
    "   data_dir='data/batches/',\n",
    "    mode='train', max_seq_length=max_seq_length, min_seq_length=min_seq_length)\n",
    "\n",
    "# import model and optimization settings\n",
    "# model = RETAIN(128,128,268,False)\n",
    "\n",
    "model = RETAIN(emb_size,hid_size,num_classes,cuda_flag)\n",
    "model.release = True\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "cnt = 0\n",
    "if cuda_flag:\n",
    "    model.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "lr_list = [0.001, 0.0003, 0.0001, 0.00003, 0.00001, 0.000003, 0.000001]\n",
    "lr_counter = 0\n",
    "lr = lr_list[lr_counter]\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_list = []\n",
    "loss_mean = 0.0\n",
    "file_cnt = 0\n",
    "cnt = 0\n",
    "loss_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(D.train_list)\n",
    "while file_cnt<(epochs*len(D.train_list)):\n",
    "    idx = file_cnt%(len_train)\n",
    "    file = D.train_list[idx]\n",
    "    if file not in loss_dict:\n",
    "        loss_dict[file] = []\n",
    "    print(\"Epoch %d [%d, %d/%d] - opening file %s\" %(((file_cnt+1)/len_train), file_cnt, idx, len_train, D.train_list[idx]))\n",
    "    file_num = int(file.split('_')[1].split('.')[0])\n",
    "    D.batch_size = int(40000/file_num)\n",
    "    D.load_batch_file(file)\n",
    "    loss_list = []\n",
    "    for i in range(D.batch_count):\n",
    "        cnt+=1\n",
    "#         input_list, targets = D.get_batch()\n",
    "        input_list, targets, dates = D.get_batch()\n",
    "#         dates = Variable(torch.Tensor(dates), requires_grad=False)\n",
    "#         if cuda_flag:\n",
    "#             dates = dates.cuda()\n",
    "        start = time.time()\n",
    "        inputs = model.list_to_tensor(input_list)\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.log(outputs)\n",
    "        targets = Variable(torch.LongTensor(targets)[:,-1]) # to only use last of each sequence\n",
    "        if cuda_flag:\n",
    "            targets = targets.cuda()\n",
    "        loss = criterion(outputs.view(-1,num_classes),targets)\n",
    "        loss_list.append(loss.data[0])\n",
    "        if cnt%10==0:\n",
    "            print('[%d] %1.3f' %(cnt,loss.data[0]))\n",
    "        if cnt%500==0:\n",
    "            print(\"Saving model at %dth step\" %cnt)\n",
    "            torch.save(model.state_dict(),'data/saved_weights/retain_copy_%d.pth'%(cnt))\n",
    "            # create CPU version\n",
    "            model2 = RETAIN(emb_size,hid_size,num_classes,False)\n",
    "            if cuda_flag:\n",
    "                model.cpu()\n",
    "            model2.load_state_dict(model.state_dict())\n",
    "            torch.save(model2.state_dict(),'data/saved_weights/retain_copy_%d_cpu.pth'%(cnt))\n",
    "            del model2\n",
    "            if cuda_flag:\n",
    "                model.cuda()\n",
    "            print(\"Saving at %dth step\"%cnt)\n",
    "        # manual loss changes\n",
    "        if cnt==100:\n",
    "            lr_counter+=1\n",
    "            lr = lr_list[lr_counter]\n",
    "            opt = optim.Adam(model.parameters(),lr=lr)\n",
    "        if cnt==500:\n",
    "            lr_counter+=1\n",
    "            lr = lr_list[lr_counter]\n",
    "            opt = optim.Adam(model.parameters(),lr=lr)        \n",
    "        if loss.data[0]>10:\n",
    "            import sys\n",
    "            sys.exit()\n",
    "#             print(loss.data[0])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(\"Loss: %1.3f\" %np.mean(loss_list))\n",
    "    loss_dict[file].append(loss.data[0])\n",
    "    file_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new model\n",
    "model = torch.load('/home/mjc/github/EHRVis/data/saved_weights/retain_bi_14500.pth')\n",
    "model.train()\n",
    "file_cnt = 4916\n",
    "cnt = 14500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lower learning rate\n",
    "lr_counter+=1\n",
    "lr = lr_list[lr_counter]\n",
    "opt = optim.Adam(model.parameters(), lr=lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
