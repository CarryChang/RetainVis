{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "from models.data_loader import DataLoader\n",
    "from models.retain_bidirectional import RETAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 30\n",
    "batch_size = 50\n",
    "max_seq_length = 300\n",
    "min_seq_length = 5\n",
    "num_classes = 268\n",
    "emb_size = 128\n",
    "hid_size = 128\n",
    "lr = 0.001\n",
    "cuda_flag = True\n",
    "\n",
    "# data loader\n",
    "D = DataLoader(batch_size=batch_size,\n",
    "   data_dir='data/batches/',\n",
    "    mode='train', max_seq_length=max_seq_length, min_seq_length=min_seq_length)\n",
    "\n",
    "# import model and optimization settings\n",
    "model = RETAIN(emb_size,hid_size,num_classes,cuda_flag)\n",
    "model.release = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "cnt = 0\n",
    "if cuda_flag:\n",
    "    model.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "lr_list = [0.001, 0.0003, 0.0001, 0.00003, 0.00001, 0.000003, 0.000001]\n",
    "lr_counter = 0\n",
    "lr = lr_list[lr_counter]\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_list = []\n",
    "loss_mean = 0.0\n",
    "file_cnt = 0\n",
    "cnt = 0\n",
    "loss_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [727, 317/410] - opening file 2015_49.pckl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjc/github/EHRVis/models/retain_bidirectional.py:35: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  outputs1 = self.RNN1(embedded) # [b x seq x 128*2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.985\n",
      "Epoch 1 [728, 318/410] - opening file 2014_232.pckl\n",
      "Loss: 3.396\n",
      "Epoch 1 [729, 319/410] - opening file 2015_112.pckl\n",
      "Loss: 3.010\n",
      "Epoch 1 [730, 320/410] - opening file 2015_288.pckl\n",
      "[2510] 0.460\n",
      "Loss: 0.460\n",
      "Epoch 1 [731, 321/410] - opening file 2015_74.pckl\n",
      "Loss: 2.919\n",
      "Epoch 1 [732, 322/410] - opening file 2014_14.pckl\n",
      "[2520] 3.038\n",
      "[2530] 3.051\n",
      "Loss: 3.068\n",
      "Epoch 1 [733, 323/410] - opening file 2014_101.pckl\n",
      "[2540] 2.545\n",
      "Loss: 2.756\n",
      "Epoch 1 [734, 324/410] - opening file 2015_7.pckl\n",
      "[2550] 3.546\n",
      "[2560] 3.388\n",
      "Loss: 3.526\n",
      "Epoch 1 [735, 325/410] - opening file 2015_154.pckl\n",
      "Loss: 1.666\n",
      "Epoch 1 [736, 326/410] - opening file 2014_151.pckl\n",
      "Loss: 1.967\n",
      "Epoch 1 [737, 327/410] - opening file 2014_5.pckl\n",
      "[2570] 3.773\n",
      "[2580] 3.709\n",
      "Loss: 3.777\n",
      "Epoch 1 [738, 328/410] - opening file 2014_256.pckl\n",
      "Loss: 1.316\n",
      "Epoch 1 [739, 329/410] - opening file 2015_237.pckl\n",
      "Loss: 2.427\n",
      "Epoch 1 [740, 330/410] - opening file 2015_123.pckl\n",
      "Loss: 3.090\n",
      "Epoch 1 [741, 331/410] - opening file 2014_190.pckl\n",
      "Loss: 1.077\n",
      "Epoch 1 [742, 332/410] - opening file 2014_85.pckl\n",
      "Loss: 3.094\n",
      "Epoch 1 [743, 333/410] - opening file 2015_260.pckl\n",
      "Loss: 1.167\n",
      "Epoch 1 [744, 334/410] - opening file 2015_70.pckl\n",
      "[2590] 3.069\n",
      "Loss: 3.046\n",
      "Epoch 1 [745, 335/410] - opening file 2015_296.pckl\n",
      "Loss: 0.884\n",
      "Epoch 1 [746, 336/410] - opening file 2015_214.pckl\n",
      "Loss: 3.126\n",
      "Epoch 1 [747, 337/410] - opening file 2014_81.pckl\n",
      "Loss: 3.004\n",
      "Epoch 1 [748, 338/410] - opening file 2014_197.pckl\n",
      "Loss: 3.732\n",
      "Epoch 1 [749, 339/410] - opening file 2014_144.pckl\n",
      "Loss: 2.608\n",
      "Epoch 1 [750, 340/410] - opening file 2014_200.pckl\n",
      "Loss: 3.042\n",
      "Epoch 1 [751, 341/410] - opening file 2014_84.pckl\n",
      "[2600] 3.041\n",
      "Loss: 2.976\n",
      "Epoch 1 [752, 342/410] - opening file 2014_279.pckl\n",
      "Loss: 0.164\n",
      "Epoch 1 [753, 343/410] - opening file 2014_87.pckl\n",
      "Loss: 3.174\n",
      "Epoch 1 [754, 344/410] - opening file 2014_167.pckl\n",
      "Loss: 1.638\n",
      "Epoch 1 [755, 345/410] - opening file 2015_20.pckl\n",
      "[2610] 2.936\n",
      "[2620] 2.972\n",
      "Loss: 2.993\n",
      "Epoch 1 [756, 346/410] - opening file 2014_258.pckl\n",
      "Loss: 2.030\n",
      "Epoch 1 [757, 347/410] - opening file 2014_21.pckl\n",
      "[2630] 2.853\n",
      "[2640] 3.022\n",
      "Loss: 2.951\n",
      "Epoch 1 [758, 348/410] - opening file 2015_165.pckl\n",
      "Loss: 1.902\n",
      "Epoch 1 [759, 349/410] - opening file 2014_246.pckl\n",
      "Loss: 1.781\n",
      "Epoch 1 [760, 350/410] - opening file 2014_120.pckl\n",
      "[2650] 2.947\n",
      "Loss: 2.947\n",
      "Epoch 1 [761, 351/410] - opening file 2014_73.pckl\n",
      "Loss: 2.777\n",
      "Epoch 1 [762, 352/410] - opening file 2014_49.pckl\n",
      "[2660] 3.089\n",
      "Loss: 2.970\n",
      "Epoch 1 [763, 353/410] - opening file 2014_175.pckl\n",
      "Loss: 2.018\n",
      "Epoch 1 [764, 354/410] - opening file 2014_223.pckl\n",
      "Loss: 3.118\n",
      "Epoch 1 [765, 355/410] - opening file 2014_67.pckl\n",
      "Loss: 3.108\n",
      "Epoch 1 [766, 356/410] - opening file 2014_143.pckl\n",
      "Loss: 3.075\n",
      "Epoch 1 [767, 357/410] - opening file 2015_84.pckl\n",
      "Loss: 3.228\n",
      "Epoch 1 [768, 358/410] - opening file 2015_68.pckl\n",
      "[2670] 3.102\n",
      "Loss: 2.616\n",
      "Epoch 1 [769, 359/410] - opening file 2015_41.pckl\n",
      "[2680] 2.814\n",
      "Loss: 2.945\n",
      "Epoch 1 [770, 360/410] - opening file 2015_148.pckl\n",
      "Loss: 2.675\n",
      "Epoch 1 [771, 361/410] - opening file 2014_88.pckl\n",
      "Loss: 2.937\n",
      "Epoch 1 [772, 362/410] - opening file 2014_6.pckl\n",
      "[2690] 3.352\n",
      "[2700] 3.379\n",
      "Loss: 3.373\n",
      "Epoch 1 [773, 363/410] - opening file 2014_207.pckl\n",
      "Loss: 1.875\n",
      "Epoch 1 [774, 364/410] - opening file 2015_114.pckl\n",
      "Loss: 2.704\n",
      "Epoch 1 [775, 365/410] - opening file 2014_270.pckl\n",
      "Loss: 2.790\n",
      "Epoch 1 [776, 366/410] - opening file 2015_132.pckl\n",
      "Loss: 2.976\n",
      "Epoch 1 [777, 367/410] - opening file 2015_218.pckl\n",
      "[2710] 2.177\n",
      "Loss: 2.177\n",
      "Epoch 1 [778, 368/410] - opening file 2014_52.pckl\n",
      "Loss: 3.030\n",
      "Epoch 1 [779, 369/410] - opening file 2014_220.pckl\n",
      "Loss: 2.078\n",
      "Epoch 1 [780, 370/410] - opening file 2015_78.pckl\n",
      "[2720] 3.034\n",
      "Loss: 3.195\n",
      "Epoch 1 [781, 371/410] - opening file 2014_177.pckl\n",
      "Loss: 1.507\n",
      "Epoch 1 [782, 372/410] - opening file 2014_267.pckl\n",
      "Loss: 1.280\n",
      "Epoch 1 [783, 373/410] - opening file 2015_86.pckl\n",
      "Loss: 3.205\n",
      "Epoch 1 [784, 374/410] - opening file 2014_295.pckl\n",
      "Loss: 2.832\n",
      "Epoch 1 [785, 375/410] - opening file 2015_201.pckl\n",
      "Loss: 1.103\n",
      "Epoch 1 [786, 376/410] - opening file 2014_122.pckl\n",
      "Loss: 3.109\n",
      "Epoch 1 [787, 377/410] - opening file 2014_195.pckl\n",
      "Loss: 3.078\n",
      "Epoch 1 [788, 378/410] - opening file 2015_93.pckl\n",
      "[2730] 3.073\n",
      "Loss: 3.199\n",
      "Epoch 1 [789, 379/410] - opening file 2014_219.pckl\n",
      "Loss: 2.550\n",
      "Epoch 1 [790, 380/410] - opening file 2015_185.pckl\n",
      "Loss: 3.072\n",
      "Epoch 1 [791, 381/410] - opening file 2015_239.pckl\n",
      "Loss: 2.470\n",
      "Epoch 1 [792, 382/410] - opening file 2015_270.pckl\n",
      "Loss: 1.403\n",
      "Epoch 1 [793, 383/410] - opening file 2014_226.pckl\n",
      "Loss: 2.858\n",
      "Epoch 1 [794, 384/410] - opening file 2014_126.pckl\n",
      "Loss: 2.992\n",
      "Epoch 1 [795, 385/410] - opening file 2015_141.pckl\n",
      "Loss: 2.350\n",
      "Epoch 1 [796, 386/410] - opening file 2014_100.pckl\n",
      "[2740] 2.130\n",
      "Loss: 2.712\n",
      "Epoch 1 [797, 387/410] - opening file 2015_244.pckl\n",
      "Loss: 1.672\n",
      "Epoch 1 [798, 388/410] - opening file 2014_149.pckl\n",
      "Loss: 2.886\n",
      "Epoch 1 [799, 389/410] - opening file 2014_128.pckl\n",
      "Loss: 3.179\n",
      "Epoch 1 [800, 390/410] - opening file 2015_108.pckl\n",
      "Loss: 2.665\n",
      "Epoch 1 [801, 391/410] - opening file 2015_282.pckl\n",
      "Loss: 2.703\n",
      "Epoch 1 [802, 392/410] - opening file 2014_260.pckl\n",
      "Loss: 2.214\n",
      "Epoch 1 [803, 393/410] - opening file 2014_9.pckl\n",
      "[2750] 2.994\n",
      "[2760] 3.110\n",
      "Loss: 3.082\n",
      "Epoch 1 [804, 394/410] - opening file 2015_196.pckl\n",
      "[2770] 1.618\n",
      "Loss: 1.618\n",
      "Epoch 1 [805, 395/410] - opening file 2014_164.pckl\n",
      "Loss: 0.931\n",
      "Epoch 1 [806, 396/410] - opening file 2014_43.pckl\n",
      "[2780] 2.994\n",
      "Loss: 2.956\n",
      "Epoch 1 [807, 397/410] - opening file 2014_24.pckl\n",
      "[2790] 2.942\n",
      "Loss: 2.901\n",
      "Epoch 1 [808, 398/410] - opening file 2015_284.pckl\n",
      "Loss: 0.686\n",
      "Epoch 1 [809, 399/410] - opening file 2015_168.pckl\n",
      "[2800] 2.102\n",
      "Loss: 2.102\n",
      "Epoch 1 [810, 400/410] - opening file 2015_242.pckl\n",
      "Loss: 1.259\n",
      "Epoch 1 [811, 401/410] - opening file 2014_259.pckl\n",
      "Loss: 1.414\n",
      "Epoch 1 [812, 402/410] - opening file 2015_36.pckl\n",
      "[2810] 3.002\n",
      "Loss: 2.962\n",
      "Epoch 1 [813, 403/410] - opening file 2014_283.pckl\n",
      "Loss: 0.301\n",
      "Epoch 1 [814, 404/410] - opening file 2014_139.pckl\n",
      "Loss: 2.738\n",
      "Epoch 1 [815, 405/410] - opening file 2015_223.pckl\n",
      "Loss: 2.624\n",
      "Epoch 1 [816, 406/410] - opening file 2014_263.pckl\n",
      "Loss: 2.525\n",
      "Epoch 1 [817, 407/410] - opening file 2014_50.pckl\n",
      "[2820] 2.790\n",
      "Loss: 2.907\n",
      "Epoch 1 [818, 408/410] - opening file 2014_103.pckl\n",
      "Loss: 2.688\n",
      "Epoch 2 [819, 409/410] - opening file 2015_146.pckl\n",
      "Loss: 2.794\n",
      "Epoch 2 [820, 0/410] - opening file 2015_118.pckl\n"
     ]
    }
   ],
   "source": [
    "len_train = len(D.train_list)\n",
    "while file_cnt<(epochs*len(D.train_list)):\n",
    "    idx = file_cnt%(len_train)\n",
    "    file = D.train_list[idx]\n",
    "    if file not in loss_dict:\n",
    "        loss_dict[file] = []\n",
    "    print(\"Epoch %d [%d, %d/%d] - opening file %s\" %(((file_cnt+1)/len_train), file_cnt, idx, len_train, D.train_list[idx]))\n",
    "    file_num = int(file.split('_')[1].split('.')[0])\n",
    "    D.batch_size = int(20000/file_num)\n",
    "    D.load_batch_file(file)\n",
    "    loss_list = []\n",
    "    for i in range(D.batch_count):\n",
    "        cnt+=1\n",
    "        input_list, targets = D.get_batch()\n",
    "        start = time.time()\n",
    "        inputs = model.list_to_tensor(input_list)\n",
    "        outputs = model(inputs)\n",
    "        targets = Variable(torch.LongTensor(targets)[:,-1]) # to only use last of each sequence\n",
    "#             targets = Variable(torch.LongTensor(targets)).view(len(inputs),-1)[:,-1] # to only use last of each sequence\n",
    "        if cuda_flag:\n",
    "            targets = targets.cuda()\n",
    "        loss = criterion(outputs.view(-1,num_classes),targets)\n",
    "        loss_list.append(loss.data[0])\n",
    "        if cnt%10==0:\n",
    "            print('[%d] %1.3f' %(cnt,loss.data[0]))\n",
    "        if cnt%500==0:\n",
    "            print(\"Saving model at %dth step\" %cnt)\n",
    "            torch.save(model,'data/saved_weights/retain_bi_%d.pth'%(cnt))\n",
    "            # create CPU version\n",
    "            model2 = RETAIN(emb_size,hid_size,num_classes,False)\n",
    "            if cuda_flag:\n",
    "                model.cpu()\n",
    "            model2.load_state_dict(model.state_dict())\n",
    "            torch.save(model2,'data/saved_weights/retain_bi_%d_cpu.pth'%(cnt))\n",
    "            if cuda_flag:\n",
    "                model.cuda()\n",
    "            print(\"Saving at %dth step\"%cnt)\n",
    "        # manual loss changes\n",
    "        if cnt==100:\n",
    "            lr_counter+=1\n",
    "            lr = lr_list[lr_counter]\n",
    "            opt = optim.Adam(model.parameters(),lr=lr)\n",
    "        if cnt==500:\n",
    "            lr_counter+=1\n",
    "            lr = lr_list[lr_counter]\n",
    "            opt = optim.Adam(model.parameters(),lr=lr)        \n",
    "        if loss.data[0]>10:\n",
    "            import sys\n",
    "            sys.exit()\n",
    "#             print(loss.data[0])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(\"Loss: %1.3f\" %np.mean(loss_list))\n",
    "    loss_dict[file].append(loss.data[0])\n",
    "    file_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load new model\n",
    "model = torch.load('/home/mjc/github/EHRVis/data/saved_weights/retain_bi_2500.pth')\n",
    "file_cnt = 727\n",
    "cnt = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lower learning rate\n",
    "lr_counter+=1\n",
    "lr = lr_list[lr_counter]\n",
    "opt = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
